--- a/fable-collector-main/main.py
+++ b/fable-collector-main/main.py
@@ -21,79 +21,7 @@
 from urllib.parse import urlencode
 from zoneinfo import ZoneInfo
 import urllib.request
-# import math
-
-# -----------------------
-# Config & budgets
-# -----------------------
-HTTP_TIMEOUT_S       = int(os.getenv("FABLE_HTTP_TIMEOUT_S", "10"))
-HTTP_RETRIES         = int(os.getenv("FABLE_HTTP_RETRIES", "1"))
-MODEL_ORDER          = [m.strip() for m in os.getenv(
-    "FABLE_MODEL_ORDER", "icon_seamless,gfs_seamless,ecmwf_ifs04,default"
-).split(",") if m.strip()]
-SITE_BUDGET_S        = int(os.getenv("FABLE_SITE_BUDGET_S", "90"))
-HARD_BUDGET_S        = int(os.getenv("FABLE_HARD_BUDGET_S", "240"))
-
-logging.basicConfig(
-    level=os.getenv("LOG_LEVEL", "INFO"),
-    format="%(asctime)s [%(levelname)s] %(message)s",
-)
-log = logging.getLogger("fable-collector")
-
-# Modèles parallèles à tenter (en plus du primaire choisi pour hourly)
-PARALLEL_MODELS        = [m.strip() for m in os.getenv(
-    "FABLE_PARALLEL_MODELS", "ecmwf_ifs04,icon_seamless,gfs_seamless"
-).split(",") if m.strip()]
-PARALLEL_TIMEOUT_S     = int(os.getenv("FABLE_PARALLEL_TIMEOUT_S", "10"))
-PARALLEL_RETRIES       = int(os.getenv("FABLE_PARALLEL_RETRIES", "0"))  # appels parallèles = best-effort
-
-# ➕ Debug dumps activables
-DEBUG_DUMP = os.getenv("FABLE_DEBUG_DUMP", "0") == "1"
-# Extras optionnels (activables sans impacter la logique)
-INCLUDE_EXTRAS = os.getenv("FABLE_INCLUDE_EXTRAS", "0") == "1"
-EXTRA_HOURLY   = ["relative_humidity_2m", "cloud_cover"]  # variables largement supportées
-
-# Fallback astro local (optionnel)
-ASTRAL_FALLBACK = os.getenv("FABLE_ASTRAL_FALLBACK", "1") == "1"
-try:
-    from astral import Observer
-    from astral.moon import moonrise as _astral_moonrise, moonset as _astral_moonset, phase as _astral_phase
-except Exception:
-    Observer = None
-    _astral_moonrise = _astral_moonset = _astral_phase = None
-    
-log.info("Astral ready=%s", ASTRAL_FALLBACK and (Observer is not None) and (_astral_moonrise is not None))
-
-# -----------------------
-# Helpers
-# -----------------------
-def slugify(name: str) -> str:
-    import unicodedata, re
-    s = unicodedata.normalize("NFKD", name)
-    s = "".join(ch for ch in s if not unicodedata.combining(ch))
-    s = s.lower()
-    s = re.sub(r"[^a-z0-9]+", "-", s).strip("-")
-    s = re.sub(r"-{2,}", "-", s)
-    return s
-
-def http_get_json(url: str, retry: int = HTTP_RETRIES, timeout: int = HTTP_TIMEOUT_S) -> Dict[str, Any]:
-    last_err = None
-    for attempt in range(retry + 1):
-        try:
-            req = urllib.request.Request(
-                url, headers={"User-Agent": "fable-collector/1.4 (+github actions)"}
-            )
-            with urllib.request.urlopen(req, timeout=timeout) as resp:
-                if resp.status != 200:
-                    raise RuntimeError(f"HTTP {resp.status}")
-                return json.loads(resp.read().decode("utf-8"))
-        except Exception as e:
-            last_err = e
-            if attempt < retry:
-                sleep_s = 0.8 + attempt * 1.2 + random.random() * 0.5
-                log.warning("GET failed (%s). retry in %.1fs ...", e, sleep_s)
-                time.sleep(sleep_s)
-    raise RuntimeError(f"GET failed after retries: {last_err}")
+
 
 def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)
 
@@ -147,6 +75,52 @@
 log.info("Fenêtre locale %s → %s (%dh) TZ=%s | timeouts=%ss retries=%d models=%s | budgets: site=%ss, global=%ss",
          start_local.isoformat(), end_local.isoformat(), WINDOW_H, TZ_NAME,
          HTTP_TIMEOUT_S, HTTP_RETRIES, "/".join(MODEL_ORDER), SITE_BUDGET_S, HARD_BUDGET_S)
+
+import requests
+SESSION = requests.Session()
+
+def http_get_json(url: str, retry: int = HTTP_RETRIES, timeout: int = HTTP_TIMEOUT_S) -> Dict[str, Any]:
+    backoffs = [0.5, 1.0, 2.0, 4.0]
+    connect_read = (3.0, float(timeout))
+    last_err = None
+    for attempt in range(min(retry + 1, len(backoffs)+1)):
+        try:
+            r = SESSION.get(url, timeout=connect_read, headers={"User-Agent":"fable-collector/1.5 (+github actions)"})
+            r.raise_for_status()
+            return r.json()
+        except Exception as e:
+            last_err = e
+            if attempt < len(backoffs):
+                time.sleep(backoffs[attempt])
+            else:
+                break
+    raise RuntimeError(f"GET failed after retries: {last_err}")
+
+
+import requests
+SESSION = requests.Session()
+
+def http_get_json(url: str, retry: int = HTTP_RETRIES, timeout: int = HTTP_TIMEOUT_S) -> Dict[str, Any]:
+    """
+    Robust GET with session reuse, distinct connect/read timeouts,
+    and backoff schedule 0.5→1→2→4 s.
+    """
+    backoffs = [0.5, 1.0, 2.0, 4.0]
+    connect_read = (3.0, float(timeout))
+    last_err = None
+    for attempt in range(min(retry + 1, len(backoffs)+1)):
+        try:
+            r = SESSION.get(url, timeout=connect_read, headers={"User-Agent":"fable-collector/1.5 (+github actions)"})
+            r.raise_for_status()
+            return r.json()
+        except Exception as e:
+            last_err = e
+            if attempt < len(backoffs):
+                time.sleep(backoffs[attempt])
+            else:
+                break
+    raise RuntimeError(f"GET failed after retries: {last_err}")
+
 
 # --- Normalisation DAILY: forcer TZ-aware (+01:00) au format ISO minutes ---
 def _iso_min_with_tz(x: str) -> str | None:
@@ -1165,4 +1139,174 @@
 else:
     index_target.write_text(json.dumps(index_payload, ensure_ascii=False, separators=(",", ":")), encoding="utf-8")
 
+# Build windows.json aggregate (atomic)
+try:
+    build_windows_json(PUBLIC, PUBLIC / "windows.json", RULES)
+    log.info("windows.json built.")
+except Exception as e:
+    log.error("windows.json build failed: %s", e)
+
 # ---------------------------------------------------------------
+
+
+# -----------------------
+# Aggregation of Family/Expert windows into public/windows.json (atomic)
+# -----------------------
+def _hysteresis(prev_ok: bool, value: float, cap: float, hys: float) -> bool:
+    if prev_ok:
+        return value <= (cap + hys)
+    return value <= cap
+
+def _iter_segments(flags: List[bool]) -> List[Tuple[int,int]]:
+    segs=[]; cur=None
+    for i, ok in enumerate(flags):
+        if ok:
+            if cur is None: cur=[i,i]
+            else: cur[1]=i
+        else:
+            if cur is not None: segs.append((cur[0], cur[1])); cur=None
+    if cur is not None: segs.append((cur[0], cur[1]))
+    return segs
+
+def _local_hour_from_iso(iso: str) -> int:
+    # Times in our JSON are already local ("YYYY-MM-DDTHH:MM"); we extract HH safely.
+    try:
+        return int(iso[11:13])
+    except Exception:
+        return 0
+
+def _class_flags_for_model(hourly: Dict[str, Any],
+                           marine: Dict[str, Any],
+                           rules: Dict[str, Any]) -> Tuple[List[bool], List[bool]]:
+    times = hourly.get("time") or []
+    wind = hourly.get("wind_speed_10m") or []
+    gust = hourly.get("wind_gusts_10m") or []
+    hs   = marine.get("wave_height") or marine.get("hs") or []
+    # thresholds
+    fam_w  = _dget(rules, "wind.family_max_kmh", 20.0)
+    fam_hs = _dget(rules, "sea.family_max_hs_m", 0.5)
+    exp_w, exp_hs = 25.0, 0.8
+    exp_gust = _dget(rules, "shelter.anchor_gusts_allow_up_to_kmh", 34.0)
+    hys_w  = _dget(rules, "hysteresis.wind_kmh", 1.0)
+    hys_hs = _dget(rules, "hysteresis.hs_m", 0.05)
+    fam_h1 = _dget(rules, "family_hours_local.start_h", 8)
+    fam_h2 = _dget(rules, "family_hours_local.end_h", 21)
+    fam_flags=[]; exp_flags=[]
+    prev_fam=False; prev_exp=False
+    n = min(len(times), len(wind), len(gust), len(hs))
+    for i in range(n):
+        hh = _local_hour_from_iso(times[i])
+        ok_hour = (fam_h1 <= hh <= fam_h2)
+        fam_ok = ok_hour \
+                 and _hysteresis(prev_fam, float(wind[i]), float(fam_w)) \
+                 and _hysteresis(prev_fam, float(hs[i]),   float(fam_hs))
+        exp_ok = (float(gust[i]) <= float(exp_gust)) \
+                 and _hysteresis(prev_exp, float(wind[i]), float(exp_w)) \
+                 and _hysteresis(prev_exp, float(hs[i]),   float(exp_hs))
+        fam_flags.append(bool(fam_ok)); exp_flags.append(bool(exp_ok))
+        prev_fam = bool(fam_ok); prev_exp = bool(exp_ok)
+    return fam_flags, exp_flags
+
+def _aggregate_one_spot(spot: Dict[str, Any], rules: Dict[str, Any]) -> Dict[str, Any]:
+    # Primary + models for agreement on class; marine from top-level
+    primary = spot.get("forecast_primary", {}) or {}
+    marine  = spot.get("marine", {}) or {}
+    # Build per-model flags
+    model_flags = []
+    fam_flags_p, exp_flags_p = _class_flags_for_model(primary.get("hourly", {}), marine, rules)
+    model_flags.append((fam_flags_p, exp_flags_p))
+    for mname, m in (spot.get("models") or {}).items():
+        fam_m, exp_m = _class_flags_for_model(m.get("hourly", {}), marine, rules)
+        model_flags.append((fam_m, exp_m))
+    # Consensus per hour (>=2 models agree) and safety tolerance (user policy)
+    n = len(fam_flags_p)
+    fam_cons=[]; exp_cons=[]
+    safe_w_cap = 12.0; safe_hs_cap = 0.4  # per user policy (tolerant if very safe)
+    # Pull canonical series to read values for tolerance
+    wind = primary.get("hourly", {}).get("wind_speed_10m", []) or []
+    hs   = marine.get("wave_height") or marine.get("hs") or []
+    gust = primary.get("hourly", {}).get("wind_gusts_10m", []) or []
+    for i in range(n):
+        fam_votes = sum(int(flags[0][i]) for flags in model_flags if i < len(flags[0]))
+        exp_votes = sum(int(flags[1][i]) for flags in model_flags if i < len(flags[1]))
+        fam_ok = fam_votes >= 2
+        exp_ok = exp_votes >= 2
+        # safety tolerance upgrade when models disagree but conditions are very safe
+        if not fam_ok:
+            try:
+                if float(wind[i]) <= safe_w_cap and float(hs[i]) <= safe_hs_cap:
+                    fam_ok = True
+            except Exception:
+                pass
+        if not exp_ok:
+            # Expert still requires gust threshold; do not auto-upgrade
+            pass
+        fam_cons.append(fam_ok); exp_cons.append(exp_ok)
+    # Segments (contiguous)
+    fam_segs = _iter_segments(fam_cons)
+    exp_segs = _iter_segments(exp_cons)
+    # Confidence: "medium" if >=2 models agree for >=80% of segment hours, else "low".
+    def segment_confidence(segs, class_idx):
+        out = []
+        total_models = len(model_flags)
+        for a,b in segs:
+            length = b - a + 1
+            agree_hours = 0
+            for i in range(a, b+1):
+                votes = sum(int(flags[class_idx][i]) for flags in model_flags if i < len(flags[class_idx]))
+                if votes >= 2:
+                    agree_hours += 1
+            ratio = (agree_hours / max(1,length))
+            conf = "medium" if ratio >= 0.8 else "low"
+            # Safety tolerance: if all hours in segment satisfy very-safe bounds, allow "medium"
+            if conf == "low":
+                try:
+                    if all(float(wind[i]) <= safe_w_cap and float(hs[i]) <= safe_hs_cap for i in range(a,b+1)):
+                        conf = "medium"
+                except Exception:
+                    pass
+            out.append({"start_idx": a, "end_idx": b, "confidence": conf})
+        return out
+    fam_out = segment_confidence(fam_segs, 0)
+    exp_out = segment_confidence(exp_segs, 1)
+    # Map indices to times
+    times = primary.get("hourly", {}).get("time", []) or []
+    def seg_to_iso(seg):
+        a,b = seg["start_idx"], seg["end_idx"]
+        return {"start": times[a], "end": times[b], "confidence": seg["confidence"]}
+    fam_iso = [seg_to_iso(s) for s in fam_out]
+    exp_iso = [seg_to_iso(s) for s in exp_out]
+    return {
+        "dest_slug": f"{spot['meta']['slug']}.json",
+        "dest_name": spot['meta']['name'],
+        "windows": [
+            {"class": "Family", "segments": fam_iso},
+            {"class": "Expert", "segments": exp_iso},
+        ]
+    }
+
+def build_windows_json(spots_dir: Path, out_path: Path, rules: Dict[str, Any]) -> Dict[str, Any]:
+    entries = []
+    # Load spots from disk (public/*.json)
+    for p in sorted(spots_dir.glob("*.json")):
+        try:
+            data = json.loads(p.read_text(encoding="utf-8"))
+            if isinstance(data, dict) and "meta" in data and "forecast_primary" in data:
+                entries.append(_aggregate_one_spot(data, rules))
+        except Exception as e:
+            log.warning("skip %s: %s", p, e)
+    payload = {
+        "generated_at": datetime.datetime.now(tz).isoformat(),
+        "windows": entries,
+    }
+    tmp = out_path.with_suffix(".tmp.json")
+    out_path.parent.mkdir(parents=True, exist_ok=True)
+    tmp.write_text(json.dumps(payload, ensure_ascii=False, separators=(",",":")), encoding="utf-8")
+    # Guard: keep last-known if empty
+    total = sum(len(e.get("windows", [])) for e in entries)
+    if total == 0 and out_path.exists():
+        log.warning("windows aggregate empty — keeping last-known file")
+    else:
+        os.replace(tmp, out_path)  # atomic
+    return payload
+
--- a/fable-collector-main/healthcheck.yml
+++ b/fable-collector-main/healthcheck.yml
@@ -130,3 +130,35 @@
           open(os.environ.get('GITHUB_STEP_SUMMARY','/tmp/summary'),"a").write(summary+"\n")
           sys.exit(0 if ok else 1)
           PY
+
+
+      - name: Validate windows.json (non-empty & ISO)
+        shell: python
+        run: |
+          import json, os, sys, datetime, pathlib
+          p = pathlib.Path("public/windows.json")
+          ok=True; logs=[]
+          if not p.exists():
+            logs.append("❌ public/windows.json manquant")
+            ok=False
+          else:
+            try:
+              data=json.loads(p.read_text(encoding="utf-8"))
+              entries=data.get("windows",[])
+              total=sum(len(e.get("windows",[])) for e in entries)
+              if total==0:
+                logs.append("❌ windows.json vide (0 classes/segments)"); ok=False
+              for e in entries:
+                for cls in e.get("windows",[]):
+                  for seg in cls.get("segments",[]):
+                    for key in ("start","end"):
+                      try:
+                        datetime.datetime.fromisoformat(seg[key]+":00")
+                      except Exception:
+                        logs.append(f"❌ {e.get('dest_slug')}: segment {key} non ISO"); ok=False
+            except Exception as ex:
+              logs.append(f"❌ lecture windows.json échouée: {ex}"); ok=False
+          summary=("\n".join(logs) or "✅ windows.json OK")
+          print(summary)
+          open(os.environ.get('GITHUB_STEP_SUMMARY','/tmp/summary'),"a").write(summary+"\n")
+          sys.exit(0 if ok else 1)
