name: Healthcheck (fable-collector)

on:
  schedule:
    - cron: "3 23,5,11,17 * * *"   # ~ 00:03/06:03/12:03/18:03 Africa/Tunis (UTC+1)
    - cron: "5 0,6,12,18 * * *"    # ~ 01:05/07:05/13:05/19:05 UTC -> garde-fou
  workflow_dispatch: {}

permissions:
  contents: read

jobs:
  health:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout (readme for badge)
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Run validator against Pages
        env:
          INDEX_URL: "https://rbpower-hub.github.io/fable-collector/index.json"
          REQUIRED: "gammarth-port.json,sidi-bou-said.json,ghar-el-melh.json,ras-fartass.json,el-haouaria.json"
          ALIASES: "sidi-bou-said.json=sidi-bou-sa-d.json"
          LOCAL_TZ: "Africa/Tunis"
          LEEWAY_MIN: "25"
          MIN_HOURS: "48"
        run: |
          python - << 'PY'
          import json, sys, urllib.request, datetime as dt
          from zoneinfo import ZoneInfo

          IX = urllib.request.urlopen("${INDEX_URL}").read().decode("utf-8")
          idx = json.loads(IX)
          tz = ZoneInfo("${LOCAL_TZ}")
          gen = dt.datetime.fromisoformat(idx["generated_at"].replace("Z","+00:00")).astimezone(tz)
          now = dt.datetime.now(tz)
          leeway = dt.timedelta(minutes=int("${LEEWAY_MIN}"))
          ok = True
          logs = []

          # Freshness
          if (now - gen) > dt.timedelta(hours=2):  # garde-fou large
              logs.append(f"❌ generated_at trop ancien: {gen.isoformat()} vs now {now.isoformat()}")
              ok = False

          # Presence
          files = {f["path"]: f for f in idx.get("files", [])}
          req = "${REQUIRED}".split(",") if "${REQUIRED}" else []
          alias_map = {}
          for pair in ("${ALIASES}".split(",") if "${ALIASES}" else []):
              if "=" in pair:
                  k,v = pair.split("=",1); alias_map[k]=v

          for r in req:
              if r in files: continue
              if r in alias_map and alias_map[r] in files:
                  logs.append(f"ℹ️ alias utilisé pour {r}: {alias_map[r]}")
                  continue
              logs.append(f"❌ manquant dans index.json: {r}")
              ok = False

          # Spot shallow checks
          def load(url): 
              return json.loads(urllib.request.urlopen(url).read().decode("utf-8"))
          base = "${INDEX_URL}".rsplit("/",1)[0] + "/"
          for logical in req:
              p = logical
              if p not in files and logical in alias_map and alias_map[logical] in files:
                  p = alias_map[logical]
              if p not in files: 
                  continue
              try:
                  d = load(base + p)
                  h = d.get("hourly",{})
                  T = h.get("time") or []
                  HS = h.get("hs") or h.get("wave_height") or []
                  TP = h.get("tp") or h.get("wave_period") or []
                  if not (len(T)>=int("${MIN_HOURS}") and len(HS)>=int("${MIN_HOURS}") and len(TP)>=int("${MIN_HOURS}")):
                      logs.append(f"❌ {p}: séries < {${MIN_HOURS}} h")
                      ok=False
                  # quick time parse sanity on first/last
                  import itertools
                  for t in itertools.islice(T,0,1):
                      dt.datetime.fromisoformat(t.replace("Z","+00:00"))
              except Exception as e:
                  logs.append(f"❌ lecture {p} échouée: {e}")
                  ok=False

          summary = ("\n".join(logs) or "✅ Collector OK (fraîcheur + fichiers + forme minimale)")
          print(summary)
          # Step Summary in Actions UI
          open(os.environ.get('GITHUB_STEP_SUMMARY','/tmp/summary'),"a").write(summary+"\n")
          sys.exit(0 if ok else 1)
          PY
